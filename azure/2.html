<div class="row mt-5 mb-5">
	<div class="col-12">
		<div class="card" id="card">
			<div class="card-body" id="card-body">
				<div class="d-sm-flex justify-content-between align-items-center">
					<h1 class="header-title mb-0">Azure Web Apps</h1>
				</div>
				<div class="market-status-table mt-4">
            			<h5>App service plans</h5>
						</br>
							<p>When you need to create an App Service resource, such as a web app, you create or use an existing service plan. The service plan defines the amount of resources availableto you, how much automation is available to scale and back up your web app, and how highly available to make your site with staging slots and Traffic Manager</p>
							</br>
							<p> <strong>Free/Shared</strong>&mdash;Uses a shared infrastructure; offers minimal storage, and has no options for deploying different staged versions, routing of traffic, or backups. The Shared tier allows you to use a custom domain and charges for this domain.</p>
							</br>
							<p> <strong>Basic</strong>&mdash;Provides dedicated compute resources for your web app, and allows you to use SSL and manually scale the number of web app instances you run. The free/shared and basic tiers provide a good environment for you to test the Web Apps service, but I wouldn&rsquo;t recommend running any actual production or development workloads. The performance isn&rsquo;t a limiting factor, but you miss some of the automated features, such as backups and scaling.</p>
							</br>
							<p> <strong>Standard</strong>&mdash;Adds daily backups, automatic scale of web app instances, and deployment slots, and allows you to route users with Traffic Manager. This tier may be suitable for low-demand applications or development environments in which you don&rsquo;t need a large number of backups or deployment slots.</p>
							</br>
							<p> <strong>Premium</strong>&mdash;Provides more frequent backups, increased storage, and a greater number of deployment slots and instance scaling options. This tier is ideal for most production workloads.</p>
							</br>
							<p><em><strong>The case for isolation</strong></em></p>
							</br>
								<p>With PaaS solutions such as Web Apps, the infrastructure is intentionally abstracted. As the names of some of the service plan tiers imply, web apps run across a shared platform of available resources. That&rsquo;s not at all to say that web apps are insecure and that others can view your private data! But compliance or regulatory reasons may require you to run your applications in a controlled, isolated environment. Enter App Service Environments: isolated environments that let you run App Service instances such as web apps in a segmented part of an Azure data center.</p>
								<p>You control the inbound and outbound network traffic, and can implement firewalls and create virtual private network (VPN) connections back on your on-premises resources. All these infrastructure components are still largely abstracted with App Service environments, but this approach provides great balance when you want the flexibility of PaaS solutions but also want to retain some of the more fine-grained controls over the network connections traffic flow.</p>
							</br>
							<p><strong>Deployment slots, behind the scenes </strong></p>
							<p>When you swap slots, what was live in the production slot is now in the dev slot, and what was in dev is now live in production. Not all settings can swap, such as SSL settings and custom domains, but for the most part, deployment slots are a great way to stage and validate content before it goes live to your customers. You can also perform a swap with preview, which gives you the opportunity to make sure the swapped content works correctly before it&rsquo;s publicly live in production.</p>
							<p>For production use in DevOps workflows, you can also configure Auto Swap. Here, when a code commit is noted in source control such as GitHub, it can trigger a build to an Azure Web Apps deployment slot. When that build is complete and the app is ready to serve content, the deployment slots swap automatically to make the code live in production. You typically use this workflow with a test environment to review the code changes first, not to publish live straight to production!</p>
						</br>
				</div>
			</div>
		</div>
	</div>
</div>
<div class="row mt-5 mb-5">
	<div class="col-12">
		<div class="card" id="card">
			<div class="card-body" id="card-body">
				<div class="d-sm-flex justify-content-between align-items-center">
					<h1 class="header-title mb-0">Azure Storage</h1>
				</div>
				<div class="market-status-table mt-4">
					<p>The Azure Managed Disks service simplifies the approach to VM storage. Managed disks abstract away a lot of the behind-the-scenes work to give you . . . well, a disk. That&rsquo;s all you really need to care about for VMs: how big and how fast they are, and what they connect to. Throughout the book, and in all your own real-world deployments, you should always use managed disks for VMs. Managed disks are the default option, and there aren&rsquo;t a lot of good reasons to change that behavior.</p>
						</br>
					<p>Before managed disks, you had to create a uniquely named storage account, limit the number of virtual disks you created in each, and move custom disk images manually to create VMs in different regions. These types of disks are known as unmanaged disks or classic disks. The Managed Disks service removes the need for a storage account, limits you to &ldquo;only&rdquo; 50,000 disks per subscription, and lets you create VMs from a custom image across regions. You also gain the ability to create and use snapshots of disks, automatically encrypt data at rest, and use disks up to 64 TiB.</p>
						</br>
					<p>Why is this important? If you run across old documentation or blog posts, they may have you create a storage account for your VMs. Stop right there! Yes, you can convert VMs from unmanaged disks to managed disks, but if you have a clean slate, look to begin each project with managed disks from the start. The use case for unmanaged disks is more to maintain backward compatibility with existing deployments, although I argue that you should look to convert those workloads to managed disks!</p>
						</br>
					<h5>OS disks</h5>
					</br>
					</br>
				</div>
			</div>
		</div>
	</div>
</div>
